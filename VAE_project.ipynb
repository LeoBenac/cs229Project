{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8MrjpXtrX2D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm  # Import the norm module\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import simps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_IRRPKra1_"
      },
      "outputs": [],
      "source": [
        "# Define the function that relates x and z\n",
        "# def function_x(z):\n",
        "#     # Define the relationship between x and z\n",
        "#     # Here, we use f(z) = (z - 0.5)^2 as an example\n",
        "#     return (z - 0.5)**2\n",
        "\n",
        "def joint_pdf(x, z, x_variance):\n",
        "    # Calculate f(z) for given z\n",
        "    fx = function_x(z)\n",
        "    # Calculate the PDF of x given f(z) and x_variance\n",
        "    pdf_x_given_z = norm.pdf(x, loc=fx, scale=np.sqrt(x_variance))\n",
        "    # Calculate the PDF of z (uniform distribution in the range 0 to 1)\n",
        "    pdf_z = np.where((z >= 0) & (z <= 1), 1, 0)\n",
        "    # Return the product of the PDFs of x given z and the PDF of z\n",
        "    return pdf_x_given_z * pdf_z\n",
        "\n",
        "\n",
        "def get_p_z_given_x(x, x_variance):\n",
        "    # Set the values for z for which to calculate the joint PDF\n",
        "    z_values = np.linspace(0, 1, 10000)\n",
        "    # Calculate the joint PDF for x and z at the given x value\n",
        "    pdf_joint = joint_pdf(x, z_values, x_variance)\n",
        "    # Calculate the marginal PDF of x\n",
        "    pdf_x = simps(pdf_joint, x=z_values)\n",
        "    # Calculate p(z|x) using Bayes' theorem\n",
        "    pdf_z_given_x = pdf_joint / pdf_x\n",
        "    # Return p(z|x)\n",
        "    return pdf_z_given_x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_x_samples(num_samples, x_variance):\n",
        "    \"\"\"\n",
        "    Generates x samples from a normal distribution with noise variance and mean f(z).\n",
        "    \n",
        "    Args:\n",
        "        num_samples (int): Number of samples to generate.\n",
        "        x_variance (float): Variance of the normal distribution for x generation.\n",
        "        \n",
        "    Returns:\n",
        "        x_samples (np.ndarray): Generated x samples.\n",
        "    \"\"\"\n",
        "    # Generate latent variable z from uniform distribution\n",
        "    z_samples = np.random.uniform(0, 1, num_samples)\n",
        "    \n",
        "    # Calculate x mean based on z values\n",
        "    x_mean = np.where(z_samples < 0.5, (z_samples - 0.5)**2, (2*z_samples - 1.5)**2)\n",
        "    \n",
        "    print(x_mean)\n",
        "    # Generate x samples from normal distribution with calculated mean and given variance\n",
        "    x_samples = np.random.normal(x_mean, np.sqrt(x_variance), num_samples)\n",
        "    \n",
        "    return x_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def function_x(z):\n",
        "    \"\"\"\n",
        "    Calculates f(z) based on the value of z. Works for scalar or numpy array inputs.\n",
        "\n",
        "    Args:\n",
        "        z (float or numpy array): The value(s) of z.\n",
        "\n",
        "    Returns:\n",
        "        float or numpy array: The value(s) of f(z) based on the condition.\n",
        "    \"\"\"\n",
        "    # Convert z to numpy array if it's not already\n",
        "    z = np.asarray(z)\n",
        "    # Create a mask for z values less than 0.5\n",
        "    mask = z < 0.5\n",
        "    # Apply the condition to calculate f(z) for z < 0.5 and z >= 0.5\n",
        "    fx = np.where(mask, (2*z - 0.5)**2, (2*z - 1.5)**2)\n",
        "    return fx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "jFXIh1JkrpLa",
        "outputId": "f75a3862-bef5-425a-dbaf-d922373ef164"
      },
      "outputs": [],
      "source": [
        "n = 10000\n",
        "x_values = np.linspace(-1, 1, n)\n",
        "z_values = np.linspace(0, 1, n)\n",
        "\n",
        "\n",
        "# Calculate the joint PDF for x and z at each combination of x and z values\n",
        "X, Z = np.meshgrid(x_values, z_values)\n",
        "pdf_joint = joint_pdf(X, Z, x_variance=0.01)\n",
        "\n",
        "# Define a reversed colormap\n",
        "cmap = plt.cm.viridis_r\n",
        "\n",
        "# Plot the joint density using contourf with the reversed colormap\n",
        "plt.contourf(Z, X, pdf_joint, cmap=cmap)\n",
        "plt.colorbar(label='Joint PDF')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('y')\n",
        "plt.title('Joint Density of y and z')\n",
        "plt.ylim((-0.4, 0.6))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "rQ9VU3LBvRZB",
        "outputId": "b2d9a8ca-ef7d-4c8e-ed6c-af6ed336221f"
      },
      "outputs": [],
      "source": [
        "# Plot the conditional PDF of z given x for the given x and x_variance\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize = (10, 10), sharex = True)\n",
        "\n",
        "color = ['purple', 'red', 'green']\n",
        "\n",
        "for i, x in enumerate([-3, 0.2, 1][::-1]):\n",
        "\n",
        "  pdf_z_given_x = get_p_z_given_x(x, 0.01)\n",
        "  ax[i].plot(np.linspace(0, 1, n), pdf_z_given_x, color=color[i])\n",
        "  ax[i].set_title('True posterior z given y = ' + str(x), fontsize = 20)\n",
        "ax[2].set_xlabel('z', fontsize = 20)\n",
        "\n",
        "plt.subplots_adjust(hspace=.35)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kduXcvvFAArK"
      },
      "source": [
        "# True f function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "i39dlQaKQURO",
        "outputId": "c3ff417f-dcd3-4c16-cdf1-0d86d4dd0bee"
      },
      "outputs": [],
      "source": [
        "z_ = np.linspace(0, 1, 10000)\n",
        "plt.plot(z_, function_x(z_) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2OHaSrnFNA8"
      },
      "source": [
        "# Vanilla VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgdjdN-v-ahw",
        "outputId": "94a18173-b6c5-4f44-eb6f-5efb1344d719"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# Define the VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(1, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mu_layer = nn.Linear(16, 1)\n",
        "        self.logvar_layer = nn.Linear(16, 1)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z) , mu, logvar\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.MSELoss()(recon_x, x)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(model, dataloader, device, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch: {:03d} Average loss: {:.4f}'.format(epoch, train_loss / len(dataloader.dataset)))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "\n",
        "# Generate your 1D continuous dataset\n",
        "data_np = torch.from_numpy(generate_x_samples(10000, 0.01).reshape(-1, 1))\n",
        "data_tensor = torch.tensor(data_np, dtype=torch.float32)\n",
        "dataloader = data.DataLoader(data_tensor, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the VAE model\n",
        "model = VAE().to(device)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, dataloader, device, optimizer, epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxBRFicvGNXM"
      },
      "outputs": [],
      "source": [
        "def generate_samples(model, num_samples):\n",
        "    z = torch.tensor(np.random.normal(0,1 , num_samples).astype(np.float32).reshape(-1,1) )\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        samples = model.decoder(z) \n",
        "\n",
        "    return samples.cpu().numpy().reshape(-1, )\n",
        "\n",
        "def visualize_vae(model, data, num_samples=1000):\n",
        "    generated_samples = generate_samples(model, num_samples) \n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot original dataset\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(data, bins=150, alpha=1, density=True)\n",
        "    plt.title('Original Dataset\\n Mean True Data: ' +  str(np.round(np.mean(data), 4) ) )\n",
        "\n",
        "    # Plot generated samples\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(generated_samples, bins=150, alpha=1, density=True)\n",
        "    plt.title('Generated Dataset MFG-VAE\\n Mean MFG-VAE: '+  str(np.round(np.mean(generated_samples), 4 ) ) )\n",
        "\n",
        "    plt.show()\n",
        "    print('Mean True Data:', np.mean(data))\n",
        "    print('Mean generated:', np.mean(generated_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmVofM3V4hCY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj33YMU_KHl3"
      },
      "source": [
        "# Visualize the distribution X learnt by our Vanilla VAE\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "WolkMrtiF-6Q",
        "outputId": "e7f58990-5e70-4ba1-b74d-d2ffd414a4de"
      },
      "outputs": [],
      "source": [
        "visualize_vae(model, data_np.numpy().reshape(-1, ), 10000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG_q-rMPUA-"
      },
      "source": [
        "# Plot Vanilla VAE p(z|x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taghd3kwpzrk",
        "outputId": "070a257f-cce8-4d2f-b601-8266de2a7ed8"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  for x in [-3, 0.2, 1][::-1]:\n",
        "    z, mu, log_var = model.forward(torch.tensor(np.array(x).astype(np.float32).reshape(-1, 1))) \n",
        "    print(z, mu, np.exp(log_var))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "GNDx-SLIKDIy",
        "outputId": "c806510d-227e-4a90-ade2-2898d78becf1"
      },
      "outputs": [],
      "source": [
        "def plot_posterior_z_given_x(model, x_s = [-3, 0.2, 1][::-1]):\n",
        "\n",
        "  mus = []\n",
        "  vars = []\n",
        "\n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots(3, 1, figsize = (10, 10), sharex = True)\n",
        "\n",
        "  colors = ['purple', 'red', 'green']\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  for i, x in enumerate(x_s):\n",
        "    with torch.no_grad():\n",
        "      _, mu, log_var = model.forward(torch.tensor(np.array(x).astype(np.float32).reshape(-1, 1)))\n",
        "      mu = mu.numpy()[0][0]\n",
        "      var = np.exp(2 * log_var.numpy()[0][0])\n",
        "      mus.append(mu)\n",
        "      vars.append(var)\n",
        "\n",
        "\n",
        "  \n",
        "      sigma = np.sqrt(var)    # standard deviation\n",
        "\n",
        "      x_ = np.linspace(mu - 3*sigma, mu + 3*sigma, 1000)\n",
        "      y_ = (1/(np.sqrt(2*np.pi)*sigma))*np.exp(-((x_-mu)**2)/(2*var))\n",
        "\n",
        "      ax[i].plot(x_, y_, color = colors[i])\n",
        "      ax[i].set_title('Learnt posterior z given y = ' + str(x) + '\\nMean = ' + str(np.round(mu, 4))+ ' Std = ' + str(np.round(np.sqrt(var), 4)), fontsize = 20 )\n",
        "\n",
        "  ax[2].set_xlabel('z', fontsize = 20)\n",
        "\n",
        "  plt.subplots_adjust(hspace=.35)\n",
        "\n",
        "\n",
        "plot_posterior_z_given_x(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmcCtUESrHW"
      },
      "source": [
        "# Function f learnt by the vanilla VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "9hUlY1zNTN5X",
        "outputId": "21a5ea2e-a6f2-4068-8845-9fbb19c6be89"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  y_ = model.decoder(torch.tensor(z_values.astype(np.float32).reshape(-1, 1))).numpy().reshape(-1, )\n",
        "  plt.plot(z_values, y_)\n",
        "  plt.title('Learnt f by MFG-VAE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmRoT6utBCOl"
      },
      "source": [
        "# GMM Posterior VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwjWMHRI6PtM"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, n_gaussians):\n",
        "        super(Encoder, self).__init__()\n",
        "        # First, we create a fully-connected layer that takes as input our\n",
        "        # flattened images and outputs a hidden dimension.\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Next, we create two fully-connected layers that take as input the\n",
        "        # hidden dimension and output the mean and log variance of our\n",
        "        # Gaussian mixture model. We use the number of Gaussians in the\n",
        "        # mixture and the latent dimension to reshape the output of the\n",
        "        # layers into a tensor of shape (n_gaussians, latent_dim).\n",
        "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim * n_gaussians)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim * n_gaussians)\n",
        "        self.n_gaussians = n_gaussians\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First, we pass our input through the first fully-connected layer\n",
        "        # and apply a ReLU activation function. This will output a tensor\n",
        "        # with the shape (batch_size, hidden_dim).\n",
        "        h = torch.relu(self.fc1(x)) \n",
        "        # Next, we pass the output of the hidden layer through the two\n",
        "        # fully-connected layers that output the mean and log variance of\n",
        "        # the Gaussian mixture model. We reshape the output of each layer\n",
        "        # into a tensor of shape (batch_size, n_gaussians, latent_dim).\n",
        "        mean = self.fc2_mean(h).view(-1, self.n_gaussians, self.latent_dim)\n",
        "        logvar = self.fc2_logvar(h).view(-1, self.n_gaussians, self.latent_dim)\n",
        "        return mean, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "# This class defines a decoder network for a variational autoencoder.\n",
        "# It takes in the latent variable and outputs the reconstructed input.\n",
        "# The decoder is made up of two fully connected layers.\n",
        "# The first layer is a linear transformation from the latent dimension to the hidden dimension.\n",
        "# The second layer is a linear transformation from the hidden dimension to the output dimension.\n",
        "# The output is then passed through a sigmoid function to get a value between 0 and 1.\n",
        "# This is the reconstructed input.\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = torch.relu(self.fc1(z))\n",
        "        x_recon = torch.sigmoid(self.fc2(h))\n",
        "        return x_recon\n",
        "\n",
        "class VAE_GMM(nn.Module):\n",
        "# This class is for creatiang and training a variational autoencoder with \n",
        "# a Gaussian mixture posterior.\n",
        "# The encoder is a neural network with an input layer, a hidden layer, and a latent layer.\n",
        "# The decoder is a neural network with a latent layer, a hidden layer, and an output layer.\n",
        "# The Gaussian mixture prior on the latent space is specified by setting the number of Gaussian components.\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, n_gaussians):\n",
        "        super(VAE_GMM, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, n_gaussians)\n",
        "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "        self.n_gaussians = n_gaussians\n",
        "        self.latent_dim = latent_dim\n",
        "        self.mixture_logits = nn.Parameter(torch.zeros(n_gaussians))\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        # Reparameterization trick to sample from N(mu, var) from\n",
        "        # N(0,1). This is also called the \"reparameterization trick\".\n",
        "        # This trick allows us to backpropagate through the sampling process.\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        logits = torch.softmax(self.mixture_logits, dim=0)\n",
        "        z_weighted = torch.sum(z * logits.unsqueeze(0).unsqueeze(2), dim=1)\n",
        "        x_recon = self.decoder(z_weighted)\n",
        "        return x_recon, mean, logvar, logits\n",
        "\n",
        "def loss_function(x, x_recon, mean, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(x_recon, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "def train_vae_gmm(data_loader, model, optimizer, device, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for _, data in enumerate(data_loader):\n",
        "            data = data[0]\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, mean, logvar, logits = model(data)\n",
        "            loss = loss_function(data, x_recon, mean, logvar)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Loss: {train_loss / len(data_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOx_57bP6UHA",
        "outputId": "cd29bf50-de19-4a0b-f189-10b12e6f4e8f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# generate data and trainingg\n",
        "\n",
        "data = generate_x_samples(10000,0.01)\n",
        "dataset = torch.utils.data.TensorDataset(torch.Tensor(data[:,np.newaxis]))\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 100\n",
        "latent_dim = 1\n",
        "n_gaussians = 10\n",
        "\n",
        "model = VAE_GMM(input_dim, hidden_dim, latent_dim, n_gaussians).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) \n",
        "\n",
        "train_vae_gmm(data_loader, model, optimizer, device, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "JoZqNpsM6T2z",
        "outputId": "7f28c6f7-a73c-43ea-c82a-d84592f787a9"
      },
      "outputs": [],
      "source": [
        "num_samples = 10000\n",
        "def visualize_vae_gmm(model, data, num_samples):\n",
        "\n",
        "    # sample from the prior\n",
        "    generated_samples = np.random.normal(0, 1, num_samples)\n",
        "    generated_samples = model.decoder(torch.Tensor(generated_samples[:,np.newaxis])).cpu().detach().numpy() \n",
        "    #generated_samples = generated_samples + np.random.normal(0, np.sqrt(0.01), num_samples)[:,np.newaxis]\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot original dataset\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(data, bins=50, alpha=0.5, density=True)\n",
        "    plt.title(f'Original Dataset \\n Mean True Data Normal: {np.mean(data):.4f}')\n",
        "\n",
        "    # Plot generated samples\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(generated_samples, bins=50, alpha=0.5, density=True)\n",
        "    plt.title(f'Generated Dataset MoG-VAE \\n Mean Generated Data: {np.mean(generated_samples):.4f}')\n",
        "\n",
        "    plt.show()\n",
        "    print(np.mean(data))\n",
        "    print(np.mean(generated_samples))\n",
        "visualize_vae_gmm(model, data, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "ekzo4Ngb6T0g",
        "outputId": "c5d39f9f-fafd-4571-ea31-177036aa539b"
      },
      "outputs": [],
      "source": [
        "z_values = np.linspace(0, 1, n)\n",
        "z_samples = np.array(z_values)\n",
        "z_samples = z_samples.reshape(-1, 1)  # Reshape the latent variables to have shape (batch_size, z_dim)\n",
        "z_samples_tensor = torch.tensor(z_samples, dtype=torch.float32).to(next(model.parameters()).device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_decoded = model.decoder(z_samples_tensor).squeeze().numpy()\n",
        "    \n",
        "# %%\n",
        "true_x = function_x(z_samples)\n",
        "\n",
        "plt.plot(z_samples, x_decoded)\n",
        "plt.ylabel('f(z) learned') \n",
        "plt.xlabel('z')\n",
        "plt.title('Learned f(z) function by MoG-VAE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "vrmfsZtnbpoV",
        "outputId": "dec4200a-b3ec-4913-a006-fb4154bd38c1"
      },
      "outputs": [],
      "source": [
        "# Function to compute the Gaussian mixture density\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "def gaussian_mixture_density(x, means, variances, weights):\n",
        "    density = np.zeros_like(x)\n",
        "    stds_dev = 0\n",
        "    for m, v, w in zip(means, variances, weights):\n",
        "        stds_dev += w * np.sqrt(v)\n",
        "        density += w * norm.pdf(x, loc=m, scale=np.sqrt(v))\n",
        "    return density, np.mean(density), stds_dev[0]\n",
        "\n",
        "# Plot the conditional PDF of z given x for the given x and x_variance\n",
        "fig, ax = plt.subplots(3, 1, figsize = (10, 10), sharex = True)\n",
        "color = ['purple', 'red', 'green']\n",
        "for tu, i in zip([-3, 0.2, 1],[0,1,2]):\n",
        "  with torch.no_grad():\n",
        "      means = model.encoder(torch.Tensor([[tu]]))[0][0].numpy()\n",
        "      stdevs = torch.exp(0.5 * model.encoder(torch.Tensor([[tu]]))[1][0]).numpy()\n",
        "      weights = torch.nn.functional.softmax(model.mixture_logits, dim=0).numpy()\n",
        "  #y, mea, stad = gaussian_mixture_density(x, means,  stdevs, weights)\n",
        "    # Generate the x-axis values\n",
        "  \n",
        "  x = np.linspace(min(means) - 3 * max(stdevs), max(means) + 3 * max( stdevs), 1000)\n",
        "\n",
        "    # Compute the Gaussian mixture density\n",
        "  y, mea, stad = gaussian_mixture_density(x, means,  stdevs, weights)\n",
        "  #pdf_z_given_x = get_p_z_given_x(x, 0.01)\n",
        "  ax[i].plot(x, y, color=color[i])\n",
        "  ax[i].set_title(f'True posterior z given y =  {tu} \\n   Mean: {mea:.4f}, Std: {stad:.4f}', fontsize = 20)\n",
        "ax[2].set_xlabel('z', fontsize = 20)\n",
        "plt.subplots_adjust(hspace=.35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9lkC2GD6SLj"
      },
      "source": [
        "# GMM Prior VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLsQ4fLD6TiO"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        z_mean = self.fc2_mean(h)\n",
        "        z_logvar = self.fc2_logvar(h)\n",
        "        return z_mean, z_logvar\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = torch.relu(self.fc1(z))\n",
        "        x_recon = torch.sigmoid(self.fc2(h))\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class MixtureVAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, n_components):\n",
        "        super(MixtureVAE, self).__init__()\n",
        "        self.laten_dim = latent_dim\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "        self.prior_means = nn.Parameter(torch.zeros(n_components, latent_dim))\n",
        "        self.prior_logvars = nn.Parameter(torch.zeros(n_components, latent_dim))\n",
        "        self.prior_logits = nn.Parameter(torch.zeros(n_components))\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mean, z_logvar = self.encoder(x)\n",
        "        z = self.reparameterize(z_mean, z_logvar)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, z_mean, z_logvar, z\n",
        "\n",
        "    def loss_function(self, x, x_recon, z_mean, z_logvar, z):\n",
        "        recon_loss = nn.functional.binary_cross_entropy(x_recon, x, reduction='sum')\n",
        "    \n",
        "        prior_mean = torch.unsqueeze(z_mean, 1)  # Shape: (batch_size, 1, latent_dim)\n",
        "        prior_logvar = torch.unsqueeze(z_logvar, 1)  # Shape: (batch_size, 1, latent_dim)\n",
        "        prior_var = torch.exp(self.prior_logvars)  # Shape: (n_components, latent_dim)\n",
        "\n",
        "        z_expanded = z.unsqueeze(1)  # Shape: (batch_size, 1, latent_dim)\n",
        "        log_p_z = torch.sum(-0.5 * (torch.log(2 * torch.pi * prior_var) + (z_expanded - self.prior_means) ** 2 / prior_var), dim=2)\n",
        "        log_p_z = torch.logsumexp(log_p_z + self.prior_logits, dim=1)\n",
        "    \n",
        "        log_q_z = torch.sum(-0.5 * (torch.log(2 * torch.pi * torch.exp(z_logvar)) + (z - z_mean) ** 2 / torch.exp(z_logvar)), dim=1)\n",
        "    \n",
        "        kl_divergence = torch.sum(log_q_z - log_p_z)\n",
        "    \n",
        "        loss = recon_loss + kl_divergence\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6qs-pFU6UmI"
      },
      "outputs": [],
      "source": [
        "def train_mixture_vae(model, data_loader, epochs, learning_rate):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for _, data in enumerate(data_loader):\n",
        "            data = data[0]\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, z_mean, z_logvar, z = model(data)\n",
        "            loss = model.loss_function(data, x_recon, z_mean, z_logvar, z)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        \n",
        "        train_loss /= len(data_loader.dataset)\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs}, Train loss: {train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgPb3eB6UjV"
      },
      "outputs": [],
      "source": [
        "input_dim = 1\n",
        "hidden_dim = 100\n",
        "latent_dim = 1\n",
        "n_gaussians = 10\n",
        "model2 = MixtureVAE(input_dim, hidden_dim, latent_dim, n_gaussians)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM2rc-Fn6UhP",
        "outputId": "6a8dd52c-d537-4a0e-bc93-bc2663f486b8"
      },
      "outputs": [],
      "source": [
        "train_mixture_vae(model2, data_loader, 100, 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "0cJRf8bY6Ue9",
        "outputId": "6ea0c2ec-576b-4b8f-fbec-8d7bad68bdab"
      },
      "outputs": [],
      "source": [
        "z_samples = np.array(z_values)\n",
        "z_samples = z_samples.reshape(-1, 1)  # Reshape the latent variables to have shape (batch_size, z_dim)\n",
        "z_samples_tensor = torch.tensor(z_samples, dtype=torch.float32).to(next(model.parameters()).device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_decoded = model2.decoder(z_samples_tensor).squeeze().numpy()\n",
        "    \n",
        "plt.plot(z_samples, x_decoded)\n",
        "plt.ylabel('f(z) learned') \n",
        "plt.xlabel('z')\n",
        "plt.title('Learned f(z) function by MFG-VAE with MoG prior')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3YQQkj-b4n4"
      },
      "outputs": [],
      "source": [
        "def sample_from_gmm_prior(model, num_samples):\n",
        "    with torch.no_grad():\n",
        "        logits = torch.softmax(model.prior_logits, dim=0)\n",
        "        chosen_components = np.random.choice(model.prior_logits.shape[0], size=num_samples, p=logits.cpu().numpy())\n",
        "        z = torch.randn(num_samples, model.laten_dim).to(model.prior_logits.device)\n",
        "        z_weighted = z * logits[chosen_components].unsqueeze(1)\n",
        "        return z_weighted\n",
        "\n",
        "def generate_samples(model, num_samples):\n",
        "    z_weighted = sample_from_gmm_prior(model, num_samples)\n",
        "    with torch.no_grad():\n",
        "        samples = model.decoder(z_weighted)\n",
        "    return samples.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "SbkkSJ-hb7yc",
        "outputId": "13c771ed-9a88-4d82-e33c-edc9e94a3fd0"
      },
      "outputs": [],
      "source": [
        "num_samples = 10000\n",
        "\n",
        "def visualize_vae_gmm(model, data, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Visualizes the original data and the generated samples from the model.\n",
        "    \"\"\"\n",
        "    # Generate samples from the model\n",
        "    generated_samples = generate_samples(model, num_samples)\n",
        "    \n",
        "    # Create the figure\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot original dataset\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(data, bins=50, alpha=0.5, density=True)\n",
        "    plt.title(f'Original Dataset \\n Mean True Data Normal: {np.mean(data):.4f}')\n",
        "\n",
        "    # Plot generated samples\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(generated_samples, bins=50, alpha=0.5, density=True)\n",
        "    plt.title(f'Generated Dataset MFG-VAE with MoG prior \\n Mean Generated Data: {np.mean(generated_samples):.4f}')\n",
        "\n",
        "    plt.show()\n",
        "    print(np.mean(data))\n",
        "    print(np.mean(generated_samples))\n",
        "visualize_vae_gmm(model2, data, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "uZ0YkanMb9Rz",
        "outputId": "1a7dafcd-7c2d-44dd-e917-767b7130db95"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot the conditional PDF of z given x for the given x and x_variance\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize = (10, 10), sharex = True)\n",
        "color = ['purple', 'red', 'green']\n",
        "for tu, i in zip([-3, 0.2, 1],[0,1,2]):\n",
        "  with torch.no_grad():\n",
        "        means = model2.encoder(torch.Tensor([[tu]]))[0][0].numpy()\n",
        "        stdevs = torch.exp(0.5 * model.encoder(torch.Tensor([[tu]]))[1][0]).numpy()\n",
        "  \n",
        "  x = np.linspace(means[0]- 3 * stdevs[0][0], means[0] + 3 * stdevs[0][0], 1000)\n",
        "\n",
        "  print(stdevs[0][0])\n",
        "    # Compute the Gaussian mixture density\n",
        "  y = pdf = (1 / (stdevs[0][0] * np.sqrt(2 * np.pi))) * np.exp(- (x - means[0]) ** 2 / (2 * stdevs[0][0] ** 2))\n",
        "  ax[i].plot(x, y, color=color[i])\n",
        "  ax[i].set_title(f'True posterior z given y =  {tu} \\n   Mean: {means[0]:.4f}, Std: {stdevs[0][0]:.4f}', fontsize = 20)\n",
        "ax[2].set_xlabel('z', fontsize = 20)\n",
        "plt.subplots_adjust(hspace=.35)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
